# Galician BERT Checkpoints

This repository includes several training checkpoints of the _base_ and _small_ BERT models for Galician published by [Garcia, M. (2021) _Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy_ (ACL 2021)](https://aclanthology.org/2021.acl-long.281/), and available at the HuggingFace's hub: [BERT-base](https://huggingface.co/marcosgg/bert-base-gl-cased), and [BERT-small](https://huggingface.co/marcosgg/bert-small-gl-cased).

## 0-425k steps
The following links contain the checkpoints released by [de-Dios-Flores, I & M. Garcia (2022) _A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time_ (SEPLN 2022)](https://sepln2022.grupolys.org/):
  1. BERT-base: [zenodo link]().
  2. BERT-small: [zenodo link]().

### Citation
If you use these checkpoints please cite the following paper:

```
@article{dediosflores-garcia-2022-computational,,
    title = "A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time",
    author = "Iria {de-Dios-Flores} and Marcos Garcia",
    journal = "Procesamiento del Lenguaje Natural",
    year = "2022",
    publisher = "Sociedad Espa√±ola para el Procesamiento del Lenguaje Natural",
    volume = "69"
}
```
